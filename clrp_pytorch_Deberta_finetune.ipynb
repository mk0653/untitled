{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.10"
    },
    "papermill": {
      "default_parameters": {},
      "duration": 2256.176802,
      "end_time": "2021-06-29T11:01:40.042694",
      "environment_variables": {},
      "exception": null,
      "input_path": "__notebook__.ipynb",
      "output_path": "__notebook__.ipynb",
      "parameters": {},
      "start_time": "2021-06-29T10:24:03.865892",
      "version": "2.3.3"
    },
    "colab": {
      "name": "clrp-pytorch-Deberta-finetune.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mk0653/untitled/blob/master/clrp_pytorch_Deberta_finetune.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "papermill": {
          "duration": 0.009695,
          "end_time": "2021-06-29T10:24:10.051712",
          "exception": false,
          "start_time": "2021-06-29T10:24:10.042017",
          "status": "completed"
        },
        "tags": [],
        "id": "emerging-plasma"
      },
      "source": [
        "This notebook uses the model created in pretrain any model notebook.\n",
        "\n",
        "1. Pretrain Roberta Model: https://www.kaggle.com/maunish/clrp-pytorch-roberta-pretrain\n",
        "2. Finetune Roberta Model: this notebook, <br/>\n",
        "   Finetune Roberta Model TPU: https://www.kaggle.com/maunish/clrp-pytorch-roberta-finetune-tpu\n",
        "3. Inference Notebook: https://www.kaggle.com/maunish/clrp-pytorch-roberta-inference\n",
        "4. Roberta + SVM: https://www.kaggle.com/maunish/clrp-roberta-svm"
      ],
      "id": "emerging-plasma"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pu-82POaoX5g",
        "outputId": "8947eeab-3364-4b54-92fd-46ad1f918650"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "id": "Pu-82POaoX5g",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dz34J5SpoUaP",
        "outputId": "05fa6506-5263-4ff2-9edd-b5a7a7dad6e2"
      },
      "source": [
        "!pip install transformers"
      ],
      "id": "dz34J5SpoUaP",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.10.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.2)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: huggingface-hub>=0.0.12 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.17)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.8.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (21.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (5.4.1)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.45)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.0.12->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.5.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_kg_hide-output": true,
        "execution": {
          "iopub.execute_input": "2021-06-29T10:24:10.089299Z",
          "iopub.status.busy": "2021-06-29T10:24:10.079098Z",
          "iopub.status.idle": "2021-06-29T10:24:17.998107Z",
          "shell.execute_reply": "2021-06-29T10:24:17.997452Z"
        },
        "papermill": {
          "duration": 7.937911,
          "end_time": "2021-06-29T10:24:17.998256",
          "exception": false,
          "start_time": "2021-06-29T10:24:10.060345",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "preceding-female",
        "outputId": "0fa34f86-1715-43a9-8685-56ea143999af"
      },
      "source": [
        "!pip install accelerate"
      ],
      "id": "preceding-female",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.7/dist-packages (0.4.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from accelerate) (5.4.1)\n",
            "Requirement already satisfied: torch>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from accelerate) (1.9.0+cu102)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.4.0->accelerate) (3.7.4.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1DbzwTuOrxH5",
        "outputId": "d62f0b5a-4798-4245-96f4-8e4fb48fd689"
      },
      "source": [
        "pip install colorama"
      ],
      "id": "1DbzwTuOrxH5",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: colorama in /usr/local/lib/python3.7/dist-packages (0.4.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_kg_hide-input": true,
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "execution": {
          "iopub.execute_input": "2021-06-29T10:24:18.026363Z",
          "iopub.status.busy": "2021-06-29T10:24:18.025649Z",
          "iopub.status.idle": "2021-06-29T10:24:25.101775Z",
          "shell.execute_reply": "2021-06-29T10:24:25.100800Z"
        },
        "papermill": {
          "duration": 7.093062,
          "end_time": "2021-06-29T10:24:25.101946",
          "exception": false,
          "start_time": "2021-06-29T10:24:18.008884",
          "status": "completed"
        },
        "tags": [],
        "id": "stuck-tribune"
      },
      "source": [
        "import os\n",
        "import gc\n",
        "import sys\n",
        "import math\n",
        "import time\n",
        "import tqdm\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from accelerate import Accelerator\n",
        "from transformers import (AutoModel,AutoConfig,\n",
        "                          AutoTokenizer,get_cosine_schedule_with_warmup)\n",
        "\n",
        "from colorama import Fore, Back, Style\n",
        "r_ = Fore.RED\n",
        "b_ = Fore.BLUE\n",
        "c_ = Fore.CYAN\n",
        "g_ = Fore.GREEN\n",
        "y_ = Fore.YELLOW\n",
        "m_ = Fore.MAGENTA\n",
        "sr_ = Style.RESET_ALL"
      ],
      "id": "stuck-tribune",
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-06-29T10:24:25.129877Z",
          "iopub.status.busy": "2021-06-29T10:24:25.129338Z",
          "iopub.status.idle": "2021-06-29T10:24:25.231432Z",
          "shell.execute_reply": "2021-06-29T10:24:25.230993Z"
        },
        "papermill": {
          "duration": 0.11933,
          "end_time": "2021-06-29T10:24:25.231557",
          "exception": false,
          "start_time": "2021-06-29T10:24:25.112227",
          "status": "completed"
        },
        "tags": [],
        "id": "colonial-daniel"
      },
      "source": [
        "train_data = pd.read_csv('/content/drive/MyDrive/[NLP]医療論文仕分けコンペ/train.csv')\n",
        "test_data = pd.read_csv('/content/drive/MyDrive/[NLP]医療論文仕分けコンペ/test.csv')\n",
        "sample = pd.read_csv('/content/drive/MyDrive/[NLP]医療論文仕分けコンペ/sample_submit.csv')\n",
        "\n",
        "\n",
        "train_data['abstract'] = train_data['title'].str.cat(train_data['abstract'], na_rep='')\n",
        "\n",
        "num_bins = int(np.floor(1 + np.log2(len(train_data))))\n",
        "train_data.loc[:,'bins'] = pd.cut(train_data['judgement'],bins=num_bins,labels=False)\n",
        "\n",
        "bins = train_data.bins.to_numpy()\n",
        "target = train_data.judgement.to_numpy()\n",
        "\n",
        "def rmse_score(y_true,y_pred):\n",
        "    return np.sqrt(mean_squared_error(y_true,y_pred))"
      ],
      "id": "colonial-daniel",
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-06-29T10:24:25.258532Z",
          "iopub.status.busy": "2021-06-29T10:24:25.258046Z",
          "iopub.status.idle": "2021-06-29T10:24:25.272886Z",
          "shell.execute_reply": "2021-06-29T10:24:25.272451Z"
        },
        "papermill": {
          "duration": 0.031484,
          "end_time": "2021-06-29T10:24:25.272999",
          "exception": false,
          "start_time": "2021-06-29T10:24:25.241515",
          "status": "completed"
        },
        "tags": [],
        "id": "collaborative-consortium"
      },
      "source": [
        "config = {\n",
        "    'lr': 2e-5,\n",
        "    'wd':0.01,\n",
        "    'batch_size':8,\n",
        "    'valid_step':10,\n",
        "    'max_len':256,\n",
        "    'epochs':3,\n",
        "    'nfolds':5,\n",
        "    'seed':42,\n",
        "    'model_path':'/content/drive/MyDrive/roberta_large',\n",
        "}\n",
        "\n",
        "for i in range(config['nfolds']):\n",
        "    os.makedirs(f'/content/drive/MyDrive/model{i}',exist_ok=True)\n",
        "\n",
        "def seed_everything(seed=42):\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONASSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "\n",
        "seed_everything(seed=config['seed'])\n",
        "\n",
        "train_data['Fold'] = -1\n",
        "kfold = StratifiedKFold(n_splits=config['nfolds'],shuffle=True,random_state=config['seed'])\n",
        "for k , (train_idx,valid_idx) in enumerate(kfold.split(X=train_data,y=bins)):\n",
        "    train_data.loc[valid_idx,'Fold'] = k"
      ],
      "id": "collaborative-consortium",
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-06-29T10:24:25.305972Z",
          "iopub.status.busy": "2021-06-29T10:24:25.305298Z",
          "iopub.status.idle": "2021-06-29T10:24:25.324918Z",
          "shell.execute_reply": "2021-06-29T10:24:25.326172Z"
        },
        "papermill": {
          "duration": 0.043701,
          "end_time": "2021-06-29T10:24:25.326369",
          "exception": false,
          "start_time": "2021-06-29T10:24:25.282668",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "filled-wings",
        "outputId": "87b5ee11-8547-441d-ebec-f576cc3de9df"
      },
      "source": [
        "train_data.head()"
      ],
      "id": "filled-wings",
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>title</th>\n",
              "      <th>abstract</th>\n",
              "      <th>judgement</th>\n",
              "      <th>bins</th>\n",
              "      <th>Fold</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>One-year age changes in MRI brain volumes in o...</td>\n",
              "      <td>One-year age changes in MRI brain volumes in o...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>Supportive CSF biomarker evidence to enhance t...</td>\n",
              "      <td>Supportive CSF biomarker evidence to enhance t...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>Occurrence of basal ganglia germ cell tumors w...</td>\n",
              "      <td>Occurrence of basal ganglia germ cell tumors w...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>New developments in diagnosis and therapy of C...</td>\n",
              "      <td>New developments in diagnosis and therapy of C...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>Prolonged shedding of SARS-CoV-2 in an elderly...</td>\n",
              "      <td>Prolonged shedding of SARS-CoV-2 in an elderly...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   id                                              title  ... bins  Fold\n",
              "0   0  One-year age changes in MRI brain volumes in o...  ...    0     1\n",
              "1   1  Supportive CSF biomarker evidence to enhance t...  ...    0     1\n",
              "2   2  Occurrence of basal ganglia germ cell tumors w...  ...    0     1\n",
              "3   3  New developments in diagnosis and therapy of C...  ...    0     2\n",
              "4   4  Prolonged shedding of SARS-CoV-2 in an elderly...  ...    0     1\n",
              "\n",
              "[5 rows x 6 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-06-29T10:24:25.384427Z",
          "iopub.status.busy": "2021-06-29T10:24:25.376670Z",
          "iopub.status.idle": "2021-06-29T10:24:25.688227Z",
          "shell.execute_reply": "2021-06-29T10:24:25.689221Z"
        },
        "papermill": {
          "duration": 0.343617,
          "end_time": "2021-06-29T10:24:25.689422",
          "exception": false,
          "start_time": "2021-06-29T10:24:25.345805",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 382
        },
        "id": "economic-fifteen",
        "outputId": "b6a60da3-a7a2-4806-f57c-a4a3df116357"
      },
      "source": [
        "plt.figure(dpi=100)\n",
        "sns.countplot(train_data.bins);"
      ],
      "id": "economic-fifteen",
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 600x400 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-06-29T10:24:25.734510Z",
          "iopub.status.busy": "2021-06-29T10:24:25.733250Z",
          "iopub.status.idle": "2021-06-29T10:24:25.737199Z",
          "shell.execute_reply": "2021-06-29T10:24:25.737811Z"
        },
        "papermill": {
          "duration": 0.030428,
          "end_time": "2021-06-29T10:24:25.737981",
          "exception": false,
          "start_time": "2021-06-29T10:24:25.707553",
          "status": "completed"
        },
        "tags": [],
        "id": "convenient-swimming"
      },
      "source": [
        "class CLRPDataset(Dataset):\n",
        "    def __init__(self,df,tokenizer,max_len=128):\n",
        "        self.excerpt = df['abstract'].to_numpy()\n",
        "        self.targets = df['judgement'].to_numpy()\n",
        "        self.max_len = max_len\n",
        "        self.tokenizer = tokenizer\n",
        "    \n",
        "    def __getitem__(self,idx):\n",
        "        encode = self.tokenizer(self.excerpt[idx],\n",
        "                                return_tensors='pt',\n",
        "                                max_length=self.max_len,\n",
        "                                padding='max_length',\n",
        "                                truncation=True)\n",
        "        \n",
        "        target = torch.tensor(self.targets[idx],dtype=torch.float) \n",
        "        return encode, target\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.excerpt)"
      ],
      "id": "convenient-swimming",
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-06-29T10:24:25.783086Z",
          "iopub.status.busy": "2021-06-29T10:24:25.782342Z",
          "iopub.status.idle": "2021-06-29T10:24:25.785523Z",
          "shell.execute_reply": "2021-06-29T10:24:25.786125Z"
        },
        "papermill": {
          "duration": 0.030008,
          "end_time": "2021-06-29T10:24:25.786294",
          "exception": false,
          "start_time": "2021-06-29T10:24:25.756286",
          "status": "completed"
        },
        "tags": [],
        "id": "varied-success"
      },
      "source": [
        "class AttentionHead(nn.Module):\n",
        "    def __init__(self, in_features, hidden_dim):\n",
        "        super().__init__()\n",
        "        self.in_features = in_features\n",
        "        self.middle_features = hidden_dim\n",
        "        self.W = nn.Linear(in_features, hidden_dim)\n",
        "        self.V = nn.Linear(hidden_dim, 1)\n",
        "        self.out_features = hidden_dim\n",
        "\n",
        "    def forward(self, features):\n",
        "        att = torch.tanh(self.W(features))\n",
        "        score = self.V(att)\n",
        "        attention_weights = torch.softmax(score, dim=1)\n",
        "        context_vector = attention_weights * features\n",
        "        context_vector = torch.sum(context_vector, dim=1)\n",
        "\n",
        "        return context_vector"
      ],
      "id": "varied-success",
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-06-29T10:24:25.829934Z",
          "iopub.status.busy": "2021-06-29T10:24:25.829279Z",
          "iopub.status.idle": "2021-06-29T10:24:25.832173Z",
          "shell.execute_reply": "2021-06-29T10:24:25.832714Z"
        },
        "papermill": {
          "duration": 0.028745,
          "end_time": "2021-06-29T10:24:25.832873",
          "exception": false,
          "start_time": "2021-06-29T10:24:25.804128",
          "status": "completed"
        },
        "tags": [],
        "id": "worthy-banner"
      },
      "source": [
        "class Model(nn.Module):\n",
        "    def __init__(self,path):\n",
        "        super(Model,self).__init__()\n",
        "        self.albert = AutoModel.from_pretrained(path)  \n",
        "        self.config = AutoConfig.from_pretrained(path)\n",
        "        self.head = AttentionHead(self.config.hidden_size,self.config.hidden_size)\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        self.linear = nn.Linear(self.config.hidden_size,1)\n",
        "\n",
        "    def forward(self,**xb):\n",
        "        x = self.albert(**xb)[0]\n",
        "        x = self.head(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.linear(x)\n",
        "        return x"
      ],
      "id": "worthy-banner",
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-06-29T10:24:25.881814Z",
          "iopub.status.busy": "2021-06-29T10:24:25.880418Z",
          "iopub.status.idle": "2021-06-29T10:24:25.883263Z",
          "shell.execute_reply": "2021-06-29T10:24:25.882873Z"
        },
        "papermill": {
          "duration": 0.032096,
          "end_time": "2021-06-29T10:24:25.883365",
          "exception": false,
          "start_time": "2021-06-29T10:24:25.851269",
          "status": "completed"
        },
        "tags": [],
        "id": "average-symphony"
      },
      "source": [
        "def run(fold,verbose=True):\n",
        "    \n",
        "    def loss_fn(outputs,targets):\n",
        "        outputs = outputs.view(-1)\n",
        "        targets = targets.view(-1)\n",
        "        return torch.sqrt(nn.MSELoss()(outputs,targets))\n",
        "    \n",
        "    def train_and_evaluate_loop(train_loader,valid_loader,model,loss_fn,optimizer,epoch,fold,best_loss,valid_step=10,lr_scheduler=None):\n",
        "        train_loss = 0\n",
        "        for i, (inputs1,targets1) in enumerate(train_loader):\n",
        "            model.train()\n",
        "            optimizer.zero_grad()\n",
        "            inputs1 = {key:val.reshape(val.shape[0],-1) for key,val in inputs1.items()}\n",
        "            outputs1 = model(**inputs1)\n",
        "            loss1 = loss_fn(outputs1,targets1)\n",
        "            loss1.backward()\n",
        "            optimizer.step()\n",
        "            \n",
        "            train_loss += loss1.item()\n",
        "            \n",
        "            if lr_scheduler:\n",
        "                lr_scheduler.step()\n",
        "            \n",
        "            #evaluating for every valid_step\n",
        "            if (i % valid_step == 0) or ((i + 1) == len(train_loader)):\n",
        "                model.eval()\n",
        "                valid_loss = 0\n",
        "                with torch.no_grad():\n",
        "                    for j, (inputs2,targets2) in enumerate(valid_loader):\n",
        "                        inputs2 = {key:val.reshape(val.shape[0],-1) for key,val in inputs2.items()}\n",
        "                        outputs2 = model(**inputs2)\n",
        "                        loss2 = loss_fn(outputs2,targets2)\n",
        "                        valid_loss += loss2.item()\n",
        "                     \n",
        "                    valid_loss /= len(valid_loader)\n",
        "                    if valid_loss <= best_loss:\n",
        "                        if verbose:\n",
        "                            print(f\"epoch:{epoch} | Train Loss:{train_loss/(i+1)} | Validation loss:{valid_loss}\")\n",
        "                            print(f\"{g_}Validation loss Decreased from {best_loss} to {valid_loss}{sr_}\")\n",
        "\n",
        "                        best_loss = valid_loss\n",
        "                        torch.save(model.state_dict(),f'/content/drive/MyDrive/model{fold}/model{fold}.bin')\n",
        "                        tokenizer.save_pretrained(f'/content/drive/MyDrive/model{fold}')\n",
        "                        \n",
        "        return best_loss\n",
        "    \n",
        "    accelerator = Accelerator()\n",
        "    print(f\"{accelerator.device} is used\")\n",
        "    \n",
        "    x_train,x_valid = train_data.query(f\"Fold != {fold}\"),train_data.query(f\"Fold == {fold}\")\n",
        "    \n",
        "    tokenizer = AutoTokenizer.from_pretrained(config['model_path'])\n",
        "    model = Model(config['model_path'])\n",
        "\n",
        "    train_ds = CLRPDataset(x_train,tokenizer,config['max_len'])\n",
        "    train_dl = DataLoader(train_ds,\n",
        "                        batch_size = config[\"batch_size\"],\n",
        "                        shuffle=True,\n",
        "                        num_workers = 4,\n",
        "                        pin_memory=True,\n",
        "                        drop_last=False)\n",
        "\n",
        "    valid_ds = CLRPDataset(x_valid,tokenizer,config['max_len'])\n",
        "    valid_dl = DataLoader(valid_ds,\n",
        "                        batch_size = config[\"batch_size\"],\n",
        "                        shuffle=False,\n",
        "                        num_workers = 4,\n",
        "                        pin_memory=True,\n",
        "                        drop_last=False)\n",
        "\n",
        "    optimizer = optim.AdamW(model.parameters(),lr=config['lr'],weight_decay=config['wd'])\n",
        "    lr_scheduler = get_cosine_schedule_with_warmup(optimizer,num_warmup_steps=0,num_training_steps= 10 * len(train_dl))\n",
        "\n",
        "    model,train_dl,valid_dl,optimizer,lr_scheduler = accelerator.prepare(model,train_dl,valid_dl,optimizer,lr_scheduler)\n",
        "\n",
        "    print(f\"Fold: {fold}\")\n",
        "    best_loss = 9999\n",
        "    for epoch in range(config[\"epochs\"]):\n",
        "        print(f\"Epoch Started:{epoch}\")\n",
        "        best_loss = train_and_evaluate_loop(train_dl,valid_dl,model,loss_fn,\n",
        "                                            optimizer,epoch,fold,best_loss,\n",
        "                                            valid_step=config['valid_step'],lr_scheduler=lr_scheduler)"
      ],
      "id": "average-symphony",
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-06-29T10:24:25.954100Z",
          "iopub.status.busy": "2021-06-29T10:24:25.952907Z",
          "iopub.status.idle": "2021-06-29T11:01:37.241407Z",
          "shell.execute_reply": "2021-06-29T11:01:37.241937Z"
        },
        "papermill": {
          "duration": 2231.346823,
          "end_time": "2021-06-29T11:01:37.242141",
          "exception": false,
          "start_time": "2021-06-29T10:24:25.895318",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "center-feedback",
        "outputId": "34198196-9f94-4d92-f6f0-3e8d3cb15f21"
      },
      "source": [
        "gc.collect()\n",
        "for f in range(config['nfolds']):\n",
        "    run(f)"
      ],
      "id": "center-feedback",
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda is used\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at /content/drive/MyDrive/roberta_large were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at /content/drive/MyDrive/roberta_large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold: 0\n",
            "Epoch Started:0\n",
            "epoch:0 | Train Loss:0.11446037888526917 | Validation loss:0.4551010862190348\n",
            "\u001b[32mValidation loss Decreased from 9999 to 0.4551010862190348\u001b[0m\n",
            "epoch:0 | Train Loss:0.1722800691019405 | Validation loss:0.11617878841480381\n",
            "\u001b[32mValidation loss Decreased from 0.4551010862190348 to 0.11617878841480381\u001b[0m\n",
            "epoch:0 | Train Loss:0.1469438017479011 | Validation loss:0.07576050343052299\n",
            "\u001b[32mValidation loss Decreased from 0.11617878841480381 to 0.07576050343052299\u001b[0m\n",
            "epoch:0 | Train Loss:0.1277356881901254 | Validation loss:0.07359179773948132\n",
            "\u001b[32mValidation loss Decreased from 0.07576050343052299 to 0.07359179773948132\u001b[0m\n",
            "epoch:0 | Train Loss:0.12605609446887348 | Validation loss:0.07054508699170027\n",
            "\u001b[32mValidation loss Decreased from 0.07359179773948132 to 0.07054508699170027\u001b[0m\n",
            "epoch:0 | Train Loss:0.11786849622487157 | Validation loss:0.0688501215949298\n",
            "\u001b[32mValidation loss Decreased from 0.07054508699170027 to 0.0688501215949298\u001b[0m\n",
            "epoch:0 | Train Loss:0.11433007628261713 | Validation loss:0.06768485967794075\n",
            "\u001b[32mValidation loss Decreased from 0.0688501215949298 to 0.06768485967794075\u001b[0m\n",
            "epoch:0 | Train Loss:0.1097744847852682 | Validation loss:0.06733509179754031\n",
            "\u001b[32mValidation loss Decreased from 0.06768485967794075 to 0.06733509179754031\u001b[0m\n",
            "epoch:0 | Train Loss:0.10751022113834882 | Validation loss:0.06730803668660842\n",
            "\u001b[32mValidation loss Decreased from 0.06733509179754031 to 0.06730803668660842\u001b[0m\n",
            "epoch:0 | Train Loss:0.10720798567383366 | Validation loss:0.06505259522648367\n",
            "\u001b[32mValidation loss Decreased from 0.06730803668660842 to 0.06505259522648367\u001b[0m\n",
            "epoch:0 | Train Loss:0.10333773043486687 | Validation loss:0.06438581918713271\n",
            "\u001b[32mValidation loss Decreased from 0.06505259522648367 to 0.06438581918713271\u001b[0m\n",
            "epoch:0 | Train Loss:0.10173119473355738 | Validation loss:0.06404295837145232\n",
            "\u001b[32mValidation loss Decreased from 0.06438581918713271 to 0.06404295837145232\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "papermill": {
          "duration": 0.034829,
          "end_time": "2021-06-29T11:01:37.310316",
          "exception": false,
          "start_time": "2021-06-29T11:01:37.275487",
          "status": "completed"
        },
        "tags": [],
        "id": "magnetic-hierarchy"
      },
      "source": [
        ""
      ],
      "id": "magnetic-hierarchy",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bz3QhBPcOVCo"
      },
      "source": [
        ""
      ],
      "id": "bz3QhBPcOVCo"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "paIGXAx3qT11"
      },
      "source": [
        ""
      ],
      "id": "paIGXAx3qT11"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8P8bzHxAvnsh"
      },
      "source": [
        "Validation loss Decreased from 0.5992907591991954 to 0.5668752615650495\n",
        "\n",
        "epoch:0 | Train Loss:0.723939770544079 | Validation loss:0.5399345523781247\n",
        "\n",
        "Validation loss Decreased from 0.5668752615650495 to 0.5399345523781247\n",
        "\n",
        "epoch:0 | Train Loss:0.6412996309847872 | Validation loss:0.5067524686455727\n",
        "\n",
        "Validation loss Decreased from 0.5399345523781247 to 0.5067524686455727\n",
        "\n",
        "Epoch Started:1\n",
        "\n",
        "epoch:1 | Train Loss:0.45839802282197134 | Validation loss:0.5031425489319695\n",
        "\n",
        "Validation loss Decreased from 0.5067524686455727 to 0.5031425489319695\n",
        "\n",
        "Epoch Started:2"
      ],
      "id": "8P8bzHxAvnsh"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2WNuy3cQ04G3"
      },
      "source": [
        ""
      ],
      "id": "2WNuy3cQ04G3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jm-veDyz04kk"
      },
      "source": [
        "cuda is used\n",
        "Some weights of the model checkpoint at /content/drive/MyDrive/clrp_deberta_large were not used when initializing DebertaModel: ['cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
        "- This IS expected if you are initializing DebertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
        "- This IS NOT expected if you are initializing DebertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
        "Fold: 2\n",
        "Epoch Started:0\n",
        "epoch:0 | Train Loss:1.5339882373809814 | Validation loss:1.1018134846653738\n",
        "Validation loss Decreased from 9999 to 1.1018134846653738\n",
        "epoch:0 | Train Loss:1.2023406787352129 | Validation loss:0.9195654329279779\n",
        "Validation loss Decreased from 1.1018134846653738 to 0.9195654329279779\n",
        "epoch:0 | Train Loss:1.0459352277574085 | Validation loss:0.770928899272227\n",
        "Validation loss Decreased from 0.9195654329279779 to 0.770928899272227\n",
        "epoch:0 | Train Loss:0.9199109639852278 | Validation loss:0.7171326644823585\n",
        "Validation loss Decreased from 0.770928899272227 to 0.7171326644823585\n",
        "epoch:0 | Train Loss:0.8771063202038044 | Validation loss:0.6371641475969637\n",
        "Validation loss Decreased from 0.7171326644823585 to 0.6371641475969637\n",
        "epoch:0 | Train Loss:0.8451492125299614 | Validation loss:0.6037529071032162\n",
        "Validation loss Decreased from 0.6371641475969637 to 0.6037529071032162\n",
        "epoch:0 | Train Loss:0.8174791937624967 | Validation loss:0.5918939384356351\n",
        "Validation loss Decreased from 0.6037529071032162 to 0.5918939384356351\n",
        "epoch:0 | Train Loss:0.7876839210371395 | Validation loss:0.5655518406083886\n",
        "Validation loss Decreased from 0.5918939384356351 to 0.5655518406083886\n",
        "epoch:0 | Train Loss:0.7167889363921008 | Validation loss:0.5522066115493506\n",
        "Validation loss Decreased from 0.5655518406083886 to 0.5522066115493506\n",
        "epoch:0 | Train Loss:0.7086261122649006 | Validation loss:0.5382329699858813\n",
        "Validation loss Decreased from 0.5522066115493506 to 0.5382329699858813\n",
        "epoch:0 | Train Loss:0.70569380937522 | Validation loss:0.5343834798730595\n",
        "Validation loss Decreased from 0.5382329699858813 to 0.5343834798730595\n",
        "epoch:0 | Train Loss:0.6954040784691888 | Validation loss:0.5265876306089717\n",
        "Validation loss Decreased from 0.5343834798730595 to 0.5265876306089717\n",
        "epoch:0 | Train Loss:0.6929470235886781 | Validation loss:0.5165272117920325\n",
        "Validation loss Decreased from 0.5265876306089717 to 0.5165272117920325\n",
        "epoch:0 | Train Loss:0.6757399174691259 | Validation loss:0.5093536971950196\n",
        "Validation loss Decreased from 0.5165272117920325 to 0.5093536971950196\n",
        "epoch:0 | Train Loss:0.6713985153270611 | Validation loss:0.5065483653419455\n",
        "Validation loss Decreased from 0.5093536971950196 to 0.5065483653419455\n",
        "Epoch Started:1\n",
        "epoch:1 | Train Loss:0.4869140400127931 | Validation loss:0.5056863585093492\n",
        "Validation loss Decreased from 0.5065483653419455 to 0.5056863585093492\n",
        "epoch:1 | Train Loss:0.4733624590969667 | Validation loss:0.49288781742814564\n",
        "Validation loss Decreased from 0.5056863585093492 to 0.49288781742814564\n",
        "epoch:1 | Train Loss:0.4644895609456967 | Validation loss:0.49227010080931893\n",
        "Validation loss Decreased from 0.49288781742814564 to 0.49227010080931893\n",
        "epoch:1 | Train Loss:0.4644904254506601 | Validation loss:0.4883344232196539\n",
        "Validation loss Decreased from 0.49227010080931893 to 0.4883344232196539\n",
        "epoch:1 | Train Loss:0.4685445793425034 | Validation loss:0.48749182273594427\n",
        "Validation loss Decreased from 0.4883344232196539 to 0.48749182273594427\n",
        "Epoch Started:2\n",
        "epoch:2 | Train Loss:0.3615002399038153 | Validation loss:0.47860990275799387\n",
        "Validation loss Decreased from 0.48749182273594427 to 0.47860990275799387\n",
        "cuda is used\n",
        "Some weights of the model checkpoint at /content/drive/MyDrive/clrp_deberta_large were not used when initializing DebertaModel: ['cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
        "- This IS expected if you are initializing DebertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
        "- This IS NOT expected if you are initializing DebertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
        "Fold: 3\n",
        "Epoch Started:0\n",
        "epoch:0 | Train Loss:1.5396836996078491 | Validation loss:1.045226339303272\n",
        "Validation loss Decreased from 9999 to 1.045226339303272\n",
        "epoch:0 | Train Loss:1.013715611262755 | Validation loss:0.9690218455564807\n",
        "Validation loss Decreased from 1.045226339303272 to 0.9690218455564807\n",
        "epoch:0 | Train Loss:1.0788483179750896 | Validation loss:0.6213600420406167\n",
        "Validation loss Decreased from 0.9690218455564807 to 0.6213600420406167\n",
        "epoch:0 | Train Loss:0.9501345003804853 | Validation loss:0.5872292184913662\n",
        "Validation loss Decreased from 0.6213600420406167 to 0.5872292184913662\n",
        "epoch:0 | Train Loss:0.7587827404833022 | Validation loss:0.5775447883446452\n",
        "Validation loss Decreased from 0.5872292184913662 to 0.5775447883446452\n",
        "epoch:0 | Train Loss:0.7545652775794056 | Validation loss:0.5408451609628301\n",
        "Validation loss Decreased from 0.5775447883446452 to 0.5408451609628301\n",
        "epoch:0 | Train Loss:0.7020426447556906 | Validation loss:0.5345618752106814\n",
        "Validation loss Decreased from 0.5408451609628301 to 0.5345618752106814\n",
        "epoch:0 | Train Loss:0.6986307737913298 | Validation loss:0.5138172133707665\n",
        "Validation loss Decreased from 0.5345618752106814 to 0.5138172133707665\n",
        "epoch:0 | Train Loss:0.6693749189796045 | Validation loss:0.4998545233212726\n",
        "Validation loss Decreased from 0.5138172133707665 to 0.4998545233212726\n",
        "epoch:0 | Train Loss:0.6634011703085446 | Validation loss:0.49178518879581506\n",
        "Validation loss Decreased from 0.4998545233212726 to 0.49178518879581506\n",
        "epoch:0 | Train Loss:0.6582495454164404 | Validation loss:0.4682268832980747\n",
        "Validation loss Decreased from 0.49178518879581506 to 0.4682268832980747\n",
        "Epoch Started:1\n",
        "epoch:1 | Train Loss:0.4571445965074085 | Validation loss:0.46480296552181244\n",
        "Validation loss Decreased from 0.4682268832980747 to 0.46480296552181244\n",
        "epoch:1 | Train Loss:0.44593468321454793 | Validation loss:0.45221721417677235\n",
        "Validation loss Decreased from 0.46480296552181244 to 0.45221721417677235\n",
        "Epoch Started:2\n",
        "cuda is used\n",
        "Some weights of the model checkpoint at /content/drive/MyDrive/clrp_deberta_large were not used when initializing DebertaModel: ['cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
        "- This IS expected if you are initializing DebertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
        "- This IS NOT expected if you are initializing DebertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
        "Fold: 4\n",
        "Epoch Started:0\n",
        "epoch:0 | Train Loss:1.3526334762573242 | Validation loss:1.043923421434953\n",
        "Validation loss Decreased from 9999 to 1.043923421434953\n",
        "epoch:0 | Train Loss:1.0277932340448552 | Validation loss:0.8878293627164733\n",
        "Validation loss Decreased from 1.043923421434953 to 0.8878293627164733\n",
        "epoch:0 | Train Loss:0.9280650743416378 | Validation loss:0.6403588732363472\n",
        "Validation loss Decreased from 0.8878293627164733 to 0.6403588732363472\n",
        "epoch:0 | Train Loss:0.8112837745517981 | Validation loss:0.5826853669025529\n",
        "Validation loss Decreased from 0.6403588732363472 to 0.5826853669025529\n",
        "epoch:0 | Train Loss:0.7802146018287281 | Validation loss:0.5356398628053951\n",
        "Validation loss Decreased from 0.5826853669025529 to 0.5356398628053951\n",
        "epoch:0 | Train Loss:0.7361272820307099 | Validation loss:0.5245677983047257\n",
        "Validation loss Decreased from 0.5356398628053951 to 0.5245677983047257\n",
        "epoch:0 | Train Loss:0.690459638299695 | Validation loss:0.521551561397566\n",
        "Validation loss Decreased from 0.5245677983047257 to 0.521551561397566\n",
        "epoch:0 | Train Loss:0.6784999489877083 | Validation loss:0.508647812268054\n",
        "Validation loss Decreased from 0.521551561397566 to 0.508647812268054\n",
        "epoch:0 | Train Loss:0.646831636120673 | Validation loss:0.49024367736468855\n",
        "Validation loss Decreased from 0.508647812268054 to 0.49024367736468855\n",
        "epoch:0 | Train Loss:0.636865869357273 | Validation loss:0.4869585412383919\n",
        "Validation loss Decreased from 0.49024367736468855 to 0.4869585412383919\n",
        "Epoch Started:1\n",
        "epoch:1 | Train Loss:0.4233155191921797 | Validation loss:0.4761068085864396\n",
        "Validation loss Decreased from 0.4869585412383919 to 0.4761068085864396\n",
        "epoch:1 | Train Loss:0.44014547505351 | Validation loss:0.4732538685727287\n",
        "Validation loss Decreased from 0.4761068085864396 to 0.4732538685727287\n",
        "Epoch Started:2\n",
        "epoch:2 | Train Loss:0.3334113076723964 | Validation loss:0.47262863248166903\n",
        "Validation loss Decreased from 0.4732538685727287 to 0.47262863248166903\n",
        "epoch:2 | Train Loss:0.33792266479704486 | Validation loss:0.46382192590496907\n",
        "Validation loss Decreased from 0.47262863248166903 to 0.46382192590496907"
      ],
      "id": "jm-veDyz04kk"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y8FmyIfi77SQ"
      },
      "source": [
        "cuda is used\n",
        "Some weights of the model checkpoint at /content/drive/MyDrive/clrp_albert_xlarge_v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.bias', 'predictions.LayerNorm.weight', 'predictions.dense.weight', 'predictions.LayerNorm.bias', 'predictions.decoder.bias', 'predictions.decoder.weight']\n",
        "- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
        "- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
        "Some weights of AlbertModel were not initialized from the model checkpoint at /content/drive/MyDrive/clrp_albert_xlarge_v2 and are newly initialized: ['albert.pooler.weight', 'albert.pooler.bias']\n",
        "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
        "Fold: 0\n",
        "Epoch Started:0\n",
        "epoch:0 | Train Loss:1.2132810354232788 | Validation loss:1.0510766250146946\n",
        "Validation loss Decreased from 9999 to 1.0510766250146946\n",
        "epoch:0 | Train Loss:1.016547972505743 | Validation loss:0.8792177195280371\n",
        "Validation loss Decreased from 1.0510766250146946 to 0.8792177195280371\n",
        "epoch:0 | Train Loss:0.8735912754422143 | Validation loss:0.7240229022334999\n",
        "Validation loss Decreased from 0.8792177195280371 to 0.7240229022334999\n",
        "epoch:0 | Train Loss:0.8084834728573189 | Validation loss:0.6978504107871526\n",
        "Validation loss Decreased from 0.7240229022334999 to 0.6978504107871526\n",
        "epoch:0 | Train Loss:0.7933886754260936 | Validation loss:0.6729309919854285\n",
        "Validation loss Decreased from 0.6978504107871526 to 0.6729309919854285\n",
        "epoch:0 | Train Loss:0.7605782218359329 | Validation loss:0.6306273710559791\n",
        "Validation loss Decreased from 0.6729309919854285 to 0.6306273710559791\n",
        "epoch:0 | Train Loss:0.7405176997774898 | Validation loss:0.6232521554953615\n",
        "Validation loss Decreased from 0.6306273710559791 to 0.6232521554953615\n",
        "epoch:0 | Train Loss:0.7188766214019018 | Validation loss:0.5952457206349977\n",
        "Validation loss Decreased from 0.6232521554953615 to 0.5952457206349977\n",
        "epoch:0 | Train Loss:0.6982866429274355 | Validation loss:0.564664643415263\n",
        "Validation loss Decreased from 0.5952457206349977 to 0.564664643415263\n",
        "epoch:0 | Train Loss:0.6819179887637431 | Validation loss:0.5485147036297221\n",
        "Validation loss Decreased from 0.564664643415263 to 0.5485147036297221\n",
        "Epoch Started:1\n",
        "epoch:1 | Train Loss:0.5511268665713649 | Validation loss:0.5440304997101636\n",
        "Validation loss Decreased from 0.5485147036297221 to 0.5440304997101636\n",
        "epoch:1 | Train Loss:0.5391701474422361 | Validation loss:0.5372833009337036\n",
        "Validation loss Decreased from 0.5440304997101636 to 0.5372833009337036\n",
        "epoch:1 | Train Loss:0.5195226620455258 | Validation loss:0.535369486875937\n",
        "Validation loss Decreased from 0.5372833009337036 to 0.535369486875937\n",
        "epoch:1 | Train Loss:0.5109942430331383 | Validation loss:0.5334647921189456\n",
        "Validation loss Decreased from 0.535369486875937 to 0.5334647921189456\n",
        "epoch:1 | Train Loss:0.5261193698977832 | Validation loss:0.5274029021531763\n",
        "Validation loss Decreased from 0.5334647921189456 to 0.5274029021531763\n",
        "epoch:1 | Train Loss:0.5193598245065737 | Validation loss:0.5151790174799906\n",
        "Validation loss Decreased from 0.5274029021531763 to 0.5151790174799906\n",
        "Epoch Started:2\n",
        "epoch:2 | Train Loss:0.3865937252763573 | Validation loss:0.507169804102938\n",
        "Validation loss Decreased from 0.5151790174799906 to 0.507169804102938\n",
        "epoch:2 | Train Loss:0.39276605328959874 | Validation loss:0.5064182791491629\n",
        "Validation loss Decreased from 0.507169804102938 to 0.5064182791491629\n",
        "cuda is used\n",
        "Some weights of the model checkpoint at /content/drive/MyDrive/clrp_albert_xlarge_v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.bias', 'predictions.LayerNorm.weight', 'predictions.dense.weight', 'predictions.LayerNorm.bias', 'predictions.decoder.bias', 'predictions.decoder.weight']\n",
        "- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
        "- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
        "Some weights of AlbertModel were not initialized from the model checkpoint at /content/drive/MyDrive/clrp_albert_xlarge_v2 and are newly initialized: ['albert.pooler.weight', 'albert.pooler.bias']\n",
        "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
        "Fold: 1\n",
        "Epoch Started:0\n",
        "epoch:0 | Train Loss:0.9406171441078186 | Validation loss:1.0655220104774958\n",
        "Validation loss Decreased from 9999 to 1.0655220104774958\n",
        "epoch:0 | Train Loss:1.099583457816731 | Validation loss:1.0174494528434646\n",
        "Validation loss Decreased from 1.0655220104774958 to 1.0174494528434646\n",
        "epoch:0 | Train Loss:1.0244464647202265 | Validation loss:0.8720986041384684\n",
        "Validation loss Decreased from 1.0174494528434646 to 0.8720986041384684\n",
        "epoch:0 | Train Loss:0.9795878068093331 | Validation loss:0.8019071087031298\n",
        "Validation loss Decreased from 0.8720986041384684 to 0.8019071087031298\n",
        "epoch:0 | Train Loss:0.9386255057846628 | Validation loss:0.6958212386554395\n",
        "Validation loss Decreased from 0.8019071087031298 to 0.6958212386554395\n",
        "epoch:0 | Train Loss:0.8437089157886193 | Validation loss:0.677840853660879\n",
        "Validation loss Decreased from 0.6958212386554395 to 0.677840853660879\n",
        "epoch:0 | Train Loss:0.8147809526450197 | Validation loss:0.6347588829591241\n",
        "Validation loss Decreased from 0.677840853660879 to 0.6347588829591241\n",
        "epoch:0 | Train Loss:0.7738258462030809 | Validation loss:0.6298477322282926\n",
        "Validation loss Decreased from 0.6347588829591241 to 0.6298477322282926\n",
        "epoch:0 | Train Loss:0.7204443794607327 | Validation loss:0.6283668237672725\n",
        "Validation loss Decreased from 0.6298477322282926 to 0.6283668237672725\n",
        "epoch:0 | Train Loss:0.6943752208302693 | Validation loss:0.6162040334352306\n",
        "Validation loss Decreased from 0.6283668237672725 to 0.6162040334352306\n",
        "epoch:0 | Train Loss:0.6800458961887057 | Validation loss:0.5960067915664592\n",
        "Validation loss Decreased from 0.6162040334352306 to 0.5960067915664592\n",
        "epoch:0 | Train Loss:0.6783558939626108 | Validation loss:0.5847520102077807\n",
        "Validation loss Decreased from 0.5960067915664592 to 0.5847520102077807\n",
        "epoch:0 | Train Loss:0.6707805169134562 | Validation loss:0.5757098531639072\n",
        "Validation loss Decreased from 0.5847520102077807 to 0.5757098531639072\n",
        "Epoch Started:1\n",
        "epoch:1 | Train Loss:0.5722205062066356 | Validation loss:0.5692170083942548\n",
        "Validation loss Decreased from 0.5757098531639072 to 0.5692170083942548\n",
        "epoch:1 | Train Loss:0.5617544236706524 | Validation loss:0.5551356335341091\n",
        "Validation loss Decreased from 0.5692170083942548 to 0.5551356335341091\n",
        "epoch:1 | Train Loss:0.559095552738975 | Validation loss:0.5463487835417331\n",
        "Validation loss Decreased from 0.5551356335341091 to 0.5463487835417331\n",
        "epoch:1 | Train Loss:0.5516854665469174 | Validation loss:0.5375950401517707\n",
        "Validation loss Decreased from 0.5463487835417331 to 0.5375950401517707\n",
        "Epoch Started:2\n",
        "epoch:2 | Train Loss:0.44385703743957894 | Validation loss:0.5236077394703744\n",
        "Validation loss Decreased from 0.5375950401517707 to 0.5236077394703744\n",
        "epoch:2 | Train Loss:0.4445870465222206 | Validation loss:0.5173325643573009\n",
        "Validation loss Decreased from 0.5236077394703744 to 0.5173325643573009\n",
        "cuda is used\n",
        "Some weights of the model checkpoint at /content/drive/MyDrive/clrp_albert_xlarge_v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.bias', 'predictions.LayerNorm.weight', 'predictions.dense.weight', 'predictions.LayerNorm.bias', 'predictions.decoder.bias', 'predictions.decoder.weight']\n",
        "- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
        "- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
        "Some weights of AlbertModel were not initialized from the model checkpoint at /content/drive/MyDrive/clrp_albert_xlarge_v2 and are newly initialized: ['albert.pooler.weight', 'albert.pooler.bias']\n",
        "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
        "Fold: 2\n",
        "Epoch Started:0\n",
        "epoch:0 | Train Loss:1.3689159154891968 | Validation loss:1.0215006911418807\n",
        "Validation loss Decreased from 9999 to 1.0215006911418807\n",
        "epoch:0 | Train Loss:1.0469979643821716 | Validation loss:0.8648192093405925\n",
        "Validation loss Decreased from 1.0215006911418807 to 0.8648192093405925\n",
        "epoch:0 | Train Loss:0.9076351651123592 | Validation loss:0.7514897593310181\n",
        "Validation loss Decreased from 0.8648192093405925 to 0.7514897593310181\n",
        "epoch:0 | Train Loss:0.8965943205741144 | Validation loss:0.7095640301704407\n",
        "Validation loss Decreased from 0.7514897593310181 to 0.7095640301704407\n",
        "epoch:0 | Train Loss:0.855297854760798 | Validation loss:0.700924825290559\n",
        "Validation loss Decreased from 0.7095640301704407 to 0.700924825290559\n",
        "epoch:0 | Train Loss:0.8030312974921993 | Validation loss:0.674967822054742\n",
        "Validation loss Decreased from 0.700924825290559 to 0.674967822054742\n",
        "epoch:0 | Train Loss:0.7670293715265062 | Validation loss:0.6149005745078476\n",
        "Validation loss Decreased from 0.674967822054742 to 0.6149005745078476\n",
        "epoch:0 | Train Loss:0.7513317436307341 | Validation loss:0.5702131581558308\n",
        "Validation loss Decreased from 0.6149005745078476 to 0.5702131581558308\n",
        "epoch:0 | Train Loss:0.6916770543760926 | Validation loss:0.5456669078746312\n",
        "Validation loss Decreased from 0.5702131581558308 to 0.5456669078746312\n",
        "epoch:0 | Train Loss:0.6659314417090091 | Validation loss:0.541403280387462\n",
        "Validation loss Decreased from 0.5456669078746312 to 0.541403280387462\n",
        "epoch:0 | Train Loss:0.6574033263014324 | Validation loss:0.5367201258179167\n",
        "Validation loss Decreased from 0.541403280387462 to 0.5367201258179167\n",
        "epoch:0 | Train Loss:0.6572100062985226 | Validation loss:0.5314437843544383\n",
        "Validation loss Decreased from 0.5367201258179167 to 0.5314437843544383\n",
        "epoch:0 | Train Loss:0.6519070534737079 | Validation loss:0.5173060367224922\n",
        "Validation loss Decreased from 0.5314437843544383 to 0.5173060367224922\n",
        "Epoch Started:1\n",
        "epoch:1 | Train Loss:0.4918090445167236 | Validation loss:0.5141464530582159\n",
        "Validation loss Decreased from 0.5173060367224922 to 0.5141464530582159\n",
        "Epoch Started:2\n",
        "epoch:2 | Train Loss:0.3975763086761747 | Validation loss:0.5097126335325376\n",
        "Validation loss Decreased from 0.5141464530582159 to 0.5097126335325376\n",
        "epoch:2 | Train Loss:0.38278972477682177 | Validation loss:0.5086808171070797\n",
        "Validation loss Decreased from 0.5097126335325376 to 0.5086808171070797\n",
        "epoch:2 | Train Loss:0.38284187127904196 | Validation loss:0.5018977440998588\n",
        "Validation loss Decreased from 0.5086808171070797 to 0.5018977440998588\n",
        "epoch:2 | Train Loss:0.3953756848476591 | Validation loss:0.49025066050005633\n",
        "Validation loss Decreased from 0.5018977440998588 to 0.49025066050005633\n",
        "epoch:2 | Train Loss:0.39388340526101995 | Validation loss:0.4869628647263621\n",
        "Validation loss Decreased from 0.49025066050005633 to 0.4869628647263621\n",
        "cuda is used\n",
        "Some weights of the model checkpoint at /content/drive/MyDrive/clrp_albert_xlarge_v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.bias', 'predictions.LayerNorm.weight', 'predictions.dense.weight', 'predictions.LayerNorm.bias', 'predictions.decoder.bias', 'predictions.decoder.weight']\n",
        "- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
        "- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
        "Some weights of AlbertModel were not initialized from the model checkpoint at /content/drive/MyDrive/clrp_albert_xlarge_v2 and are newly initialized: ['albert.pooler.weight', 'albert.pooler.bias']\n",
        "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
        "Fold: 3\n",
        "Epoch Started:0\n",
        "epoch:0 | Train Loss:0.8091953992843628 | Validation loss:1.0435537334898828\n",
        "Validation loss Decreased from 9999 to 1.0435537334898828\n",
        "epoch:0 | Train Loss:0.9331630414182489 | Validation loss:0.9483419711321173\n",
        "Validation loss Decreased from 1.0435537334898828 to 0.9483419711321173\n",
        "epoch:0 | Train Loss:0.8234872817993164 | Validation loss:0.8825650307494151\n",
        "Validation loss Decreased from 0.9483419711321173 to 0.8825650307494151\n",
        "epoch:0 | Train Loss:0.873782463612095 | Validation loss:0.7621127123564062\n",
        "Validation loss Decreased from 0.8825650307494151 to 0.7621127123564062\n",
        "epoch:0 | Train Loss:0.8344071507453918 | Validation loss:0.6640060581791569\n",
        "Validation loss Decreased from 0.7621127123564062 to 0.6640060581791569\n",
        "epoch:0 | Train Loss:0.762415522923235 | Validation loss:0.6508878633170061\n",
        "Validation loss Decreased from 0.6640060581791569 to 0.6508878633170061\n",
        "epoch:0 | Train Loss:0.7567126780328616 | Validation loss:0.6402479196098488\n",
        "Validation loss Decreased from 0.6508878633170061 to 0.6402479196098488\n",
        "epoch:0 | Train Loss:0.7435390137054108 | Validation loss:0.5700231643629746\n",
        "Validation loss Decreased from 0.6402479196098488 to 0.5700231643629746\n",
        "epoch:0 | Train Loss:0.7396178874048857 | Validation loss:0.5699580292886411\n",
        "Validation loss Decreased from 0.5700231643629746 to 0.5699580292886411\n",
        "epoch:0 | Train Loss:0.7222400827348725 | Validation loss:0.5650586548825385\n",
        "Validation loss Decreased from 0.5699580292886411 to 0.5650586548825385\n",
        "epoch:0 | Train Loss:0.7200412233822219 | Validation loss:0.5552213498404328\n",
        "Validation loss Decreased from 0.5650586548825385 to 0.5552213498404328\n",
        "epoch:0 | Train Loss:0.6954209055119788 | Validation loss:0.55254034907885\n",
        "Validation loss Decreased from 0.5552213498404328 to 0.55254034907885\n",
        "epoch:0 | Train Loss:0.6856138162420824 | Validation loss:0.5413784623985559\n",
        "Validation loss Decreased from 0.55254034907885 to 0.5413784623985559\n",
        "epoch:0 | Train Loss:0.6795114541982675 | Validation loss:0.5410246444000325\n",
        "Validation loss Decreased from 0.5413784623985559 to 0.5410246444000325\n",
        "epoch:0 | Train Loss:0.6766565798229202 | Validation loss:0.5397399109853825\n",
        "Validation loss Decreased from 0.5410246444000325 to 0.5397399109853825\n",
        "Epoch Started:1\n",
        "epoch:1 | Train Loss:0.5374819993133276 | Validation loss:0.5248952847971043\n",
        "Validation loss Decreased from 0.5397399109853825 to 0.5248952847971043\n",
        "epoch:1 | Train Loss:0.5543654570098739 | Validation loss:0.5248486659896205\n",
        "Validation loss Decreased from 0.5248952847971043 to 0.5248486659896205\n",
        "epoch:1 | Train Loss:0.5406567460521128 | Validation loss:0.5238935798406601\n",
        "Validation loss Decreased from 0.5248486659896205 to 0.5238935798406601\n",
        "epoch:1 | Train Loss:0.5391451462938337 | Validation loss:0.513818223081844\n",
        "Validation loss Decreased from 0.5238935798406601 to 0.513818223081844\n",
        "Epoch Started:2\n",
        "epoch:2 | Train Loss:0.4289773560968446 | Validation loss:0.5052631640098464\n",
        "Validation loss Decreased from 0.513818223081844 to 0.5052631640098464\n",
        "epoch:2 | Train Loss:0.43202752601509253 | Validation loss:0.5009493714487049\n",
        "Validation loss Decreased from 0.5052631640098464 to 0.5009493714487049\n",
        "epoch:2 | Train Loss:0.45103617727316614 | Validation loss:0.4967938892438378\n",
        "Validation loss Decreased from 0.5009493714487049 to 0.4967938892438378\n",
        "cuda is used\n",
        "Some weights of the model checkpoint at /content/drive/MyDrive/clrp_albert_xlarge_v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.bias', 'predictions.LayerNorm.weight', 'predictions.dense.weight', 'predictions.LayerNorm.bias', 'predictions.decoder.bias', 'predictions.decoder.weight']\n",
        "- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
        "- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
        "Some weights of AlbertModel were not initialized from the model checkpoint at /content/drive/MyDrive/clrp_albert_xlarge_v2 and are newly initialized: ['albert.pooler.weight', 'albert.pooler.bias']\n",
        "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
        "Fold: 4\n",
        "Epoch Started:0\n",
        "epoch:0 | Train Loss:1.0248457193374634 | Validation loss:1.090473430257448\n",
        "Validation loss Decreased from 9999 to 1.090473430257448\n",
        "epoch:0 | Train Loss:0.9050093537027185 | Validation loss:0.9147115572237633\n",
        "Validation loss Decreased from 1.090473430257448 to 0.9147115572237633\n",
        "epoch:0 | Train Loss:0.921067122902189 | Validation loss:0.8396302835202553\n",
        "Validation loss Decreased from 0.9147115572237633 to 0.8396302835202553\n",
        "epoch:0 | Train Loss:0.8967714915352483 | Validation loss:0.7080488041253157\n",
        "Validation loss Decreased from 0.8396302835202553 to 0.7080488041253157\n",
        "epoch:0 | Train Loss:0.8847992325701365 | Validation loss:0.6811294328998512\n",
        "Validation loss Decreased from 0.7080488041253157 to 0.6811294328998512\n",
        "epoch:0 | Train Loss:0.8399373669116224 | Validation loss:0.6705871963165175\n",
        "Validation loss Decreased from 0.6811294328998512 to 0.6705871963165175\n",
        "epoch:0 | Train Loss:0.8126827491001344 | Validation loss:0.6082563746563145\n",
        "Validation loss Decreased from 0.6705871963165175 to 0.6082563746563145\n",
        "epoch:0 | Train Loss:0.7668023714334657 | Validation loss:0.5641633368713755\n",
        "Validation loss Decreased from 0.6082563746563145 to 0.5641633368713755\n",
        "epoch:0 | Train Loss:0.7356543803451866 | Validation loss:0.5513910155900767\n",
        "Validation loss Decreased from 0.5641633368713755 to 0.5513910155900767\n",
        "epoch:0 | Train Loss:0.7241591112306941 | Validation loss:0.5463290766511165\n",
        "Validation loss Decreased from 0.5513910155900767 to 0.5463290766511165\n",
        "epoch:0 | Train Loss:0.6782628996557448 | Validation loss:0.5404662420632134\n",
        "Validation loss Decreased from 0.5463290766511165 to 0.5404662420632134\n",
        "Epoch Started:1\n",
        "epoch:1 | Train Loss:0.5401059289227476 | Validation loss:0.5381482812300534\n",
        "Validation loss Decreased from 0.5404662420632134 to 0.5381482812300534\n",
        "epoch:1 | Train Loss:0.5290245124142924 | Validation loss:0.536549665138755\n",
        "Validation loss Decreased from 0.5381482812300534 to 0.536549665138755\n",
        "epoch:1 | Train Loss:0.527797653281837 | Validation loss:0.5246327181517239\n",
        "Validation loss Decreased from 0.536549665138755 to 0.5246327181517239\n",
        "Epoch Started:2\n",
        "epoch:2 | Train Loss:0.459424969202243 | Validation loss:0.5233353496017591\n",
        "Validation loss Decreased from 0.5246327181517239 to 0.5233353496017591"
      ],
      "id": "Y8FmyIfi77SQ"
    }
  ]
}