{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "haggingface_QAtask.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMNxlMtiT4htOSFFapD3sbX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mk0653/untitled/blob/master/haggingface_QAtask.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ou3O1_7HiRvL",
        "outputId": "63d81414-915d-4daf-9cc9-48da4e2828db"
      },
      "source": [
        "pip install transformers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (3.0.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.45)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (from transformers) (0.1.96)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: tokenizers==0.8.0-rc4 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.8.0rc4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "767UbUC34V35",
        "outputId": "0553091b-6d23-4401-a4b5-c6ad97b51767"
      },
      "source": [
        "pip install transformers[\"ja\"]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers[ja] in /usr/local/lib/python3.7/dist-packages (3.0.0)\n",
            "\u001b[33m  WARNING: transformers 3.0.0 does not provide the extra 'ja'\u001b[0m\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (from transformers[ja]) (0.1.96)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers[ja]) (4.41.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers[ja]) (20.9)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from transformers[ja]) (1.19.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers[ja]) (2.23.0)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers[ja]) (0.0.45)\n",
            "Requirement already satisfied: tokenizers==0.8.0-rc4 in /usr/local/lib/python3.7/dist-packages (from transformers[ja]) (0.8.0rc4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers[ja]) (3.0.12)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers[ja]) (2019.12.20)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers[ja]) (2.4.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers[ja]) (2021.5.30)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers[ja]) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers[ja]) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers[ja]) (3.0.4)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers[ja]) (1.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers[ja]) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers[ja]) (7.1.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y_U_-RW_49Cf",
        "outputId": "4ce27abd-8829-4529-fc1a-cb83afb5c9cb"
      },
      "source": [
        "pip install fugashi[unidic-lite]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: fugashi[unidic-lite] in /usr/local/lib/python3.7/dist-packages (1.1.0)\n",
            "Requirement already satisfied: unidic-lite; extra == \"unidic-lite\" in /usr/local/lib/python3.7/dist-packages (from fugashi[unidic-lite]) (1.0.8)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_PjcHAIR5c9c",
        "outputId": "c21b4541-b1af-4688-d235-a54e096cd9ae"
      },
      "source": [
        "pip install ipadic"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: ipadic in /usr/local/lib/python3.7/dist-packages (1.0.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "feRae1kbAYhz"
      },
      "source": [
        "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
        "import torch\n",
        "\n",
        "# トークナイザーとモデルの準備\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-large-uncased-whole-word-masking-finetuned-squad\")\n",
        "model = AutoModelForQuestionAnswering.from_pretrained(\"bert-large-uncased-whole-word-masking-finetuned-squad\", return_dict=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "umtO86NEhFp-"
      },
      "source": [
        "\n",
        "# テキスト\n",
        "text = r\"\"\"\n",
        "🤗 Transformers (formerly known as pytorch-transformers and pytorch-pretrained-bert) provides general-purpose\n",
        "architectures (BERT, GPT-2, RoBERTa, XLM, DistilBert, XLNet…) for Natural Language Understanding (NLU) and Natural\n",
        "Language Generation (NLG) with over 32+ pretrained models in 100+ languages and deep interoperability between\n",
        "TensorFlow 2.0 and PyTorch.\n",
        "\"\"\"\n",
        "# 🤗 Transformers(旧称：pytorch-transformers、pytorch-pretrained-bert)は、\n",
        "# 自然言語理解（NLU）と自然言語生成（NLG）のための汎用アーキテクチャ\n",
        "# （BERT、GPT-2、RoBERTa、XLM、DistilBert、XLNet...）を100以上の言語で\n",
        "# 32以上の事前学習モデルと\n",
        "# TensorFlow 2.0とPyTorchの間の相互運用性を提供します。\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qKUGkK4GhGcX"
      },
      "source": [
        "# 質問\n",
        "questions = [\n",
        "    \"How many pretrained models are available in Transformers?\",\n",
        "    \"What does Transformers provide?\",\n",
        "    \"Transformers provides interoperability between which frameworks?\",\n",
        "]\n",
        "# Transformersには何種類の事前学習モデルがあるのか？\n",
        "# Transformersは何を提供しているのか？\n",
        "# Transformersはどのフレームワーク間の相互運用性を提供していますか？"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KF_XetAshMuc",
        "outputId": "3e8a152b-14ee-4d5c-8a9a-d0d9ccfea85c"
      },
      "source": [
        "\n",
        "for question in questions:\n",
        "    # 前処理\n",
        "    inputs = tokenizer.encode_plus(question, text, add_special_tokens=True, return_tensors=\"pt\")\n",
        "    input_ids = inputs[\"input_ids\"].tolist()[0]\n",
        "\n",
        "    # 推論\n",
        "    text_tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
        "    answer_start_scores, answer_end_scores = model(**inputs)\n",
        "\n",
        "    # 可能性の高い回答の取得\n",
        "    answer_start = torch.argmax(answer_start_scores)  \n",
        "    answer_end = torch.argmax(answer_end_scores) + 1 \n",
        "    answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(input_ids[answer_start:answer_end]))\n",
        "\n",
        "    # 出力\n",
        "    print(f\"Question: {question}\")\n",
        "    print(f\"Answer: {answer}\\n\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Question: How many pretrained models are available in Transformers?\n",
            "Answer: over 32 +\n",
            "\n",
            "Question: What does Transformers provide?\n",
            "Answer: general - purpose architectures\n",
            "\n",
            "Question: Transformers provides interoperability between which frameworks?\n",
            "Answer: tensorflow 2. 0 and pytorch\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pWWH95j0lpGg"
      },
      "source": [
        "text_news = r\"\"\"\n",
        "The collapsed condominium in Florida, southern United States, has been aging for 40 years since it was built, and a preliminary survey of the renovation work revealed that multiple cracks were found in the pillars and other parts.\n",
        "More than two days have passed since an accident in which a part of a 12-story condominium collapsed near Miami in Florida, USA, and five people were confirmed dead.\n",
        "It is not clear how many people were in the building when the condominium collapsed, but according to local authorities, 156 of the residents were out of contact and were searched by rescue teams. The activity is continuing.\n",
        "\"\"\"\n",
        "#アメリカ南部フロリダ州で崩落したマンションは、建設されてからことしで40年と老朽化していて、改修工事にむけた事前の調査では柱などに複数の亀裂が見つかっていたことがわかりました。\n",
        "#アメリカ南部フロリダ州のマイアミ近郊で12階建てのマンションの一部が崩れ落ち、5人の死亡が確認されている事故が起きてから2日余りが経過しました。\n",
        "#マンションが崩れたときに何人が建物内にいたのかは明らかになっていませんが、地元当局によりますと、居住者のうち連絡がとれなくなっている人は156人にのぼっていて、救助隊による捜索活動が続けられています。\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NqGG2Gn0nQ2V"
      },
      "source": [
        "# 質問\n",
        "questions = [\n",
        "    \"What kind of accident is it?\",\n",
        "    \"What is the cause of the accident?\",\n",
        "    \"Are there any deaths?\",\n",
        "]\n",
        "\n",
        "#どのような事故ですか？\n",
        "#事故の原因は何ですか？\n",
        "#死亡者はいますか？"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F4QMPYhD-rn5",
        "outputId": "76ca59e3-2dc1-4f40-b71f-fe9189b88738"
      },
      "source": [
        "\n",
        "for question in questions:\n",
        "    # 前処理\n",
        "    inputs = tokenizer.encode_plus(question, text_news, add_special_tokens=True, return_tensors=\"pt\")\n",
        "    input_ids = inputs[\"input_ids\"].tolist()[0]\n",
        "\n",
        "    # 推論\n",
        "    text_tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
        "    answer_start_scores, answer_end_scores = model(**inputs)\n",
        "\n",
        "    # 可能性の高い回答の取得\n",
        "    answer_start = torch.argmax(answer_start_scores)  \n",
        "    answer_end = torch.argmax(answer_end_scores) + 1 \n",
        "    answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(input_ids[answer_start:answer_end]))\n",
        "\n",
        "    # 出力\n",
        "    print(f\"Question: {question}\")\n",
        "    print(f\"Answer: {answer}\\n\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Question: What kind of accident is it?\n",
            "Answer: an accident in which a part of a 12 - story condominium collapsed\n",
            "\n",
            "Question: What is the cause of the accident?\n",
            "Answer: multiple cracks\n",
            "\n",
            "Question: Are there any deaths?\n",
            "Answer: five people were confirmed dead\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B38bsNuh-sPM"
      },
      "source": [
        "日本語のbertモデルでの検証。事前学習のみでQAタスク用のファインチューニングなし"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xr1mUcZ-nw_J",
        "outputId": "410a4cae-7b8f-4310-bb17-a229d1f28c86"
      },
      "source": [
        "from transformers import BertJapaneseTokenizer, AutoModelForQuestionAnswering\n",
        "import torch\n",
        "\n",
        "# 入力テキスト\n",
        "context = r\"\"\"\n",
        "千葉県八街市を管轄する消防などによりますと、28日午後3時半ごろ、八街市八街の路上でトラックが小学生の列に突っ込みました。警察と消防によりますと、２人が心肺停止の状態で３人が重傷だということです。\n",
        "\"\"\"\n",
        "question=\"重傷者は何名？\"\n",
        "\n",
        "model = AutoModelForQuestionAnswering.from_pretrained('cl-tohoku/bert-base-japanese-whole-word-masking')  \n",
        "tokenizer = BertJapaneseTokenizer.from_pretrained('cl-tohoku/bert-base-japanese-whole-word-masking') \n",
        "\n",
        "# 推論の実行\n",
        "inputs = tokenizer.encode_plus(question, context, add_special_tokens=True, return_tensors=\"pt\")\n",
        "input_ids = inputs[\"input_ids\"].tolist()[0]\n",
        "output = model(**inputs)\n",
        "answer_start = torch.argmax(output.start_logits)  \n",
        "answer_end = torch.argmax(output.end_logits) + 1 \n",
        "answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(input_ids[answer_start:answer_end]))\n",
        "\n",
        "# 結果出力\n",
        "print(\"質問: \"+question)\n",
        "print(\"応答: \"+answer)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at cl-tohoku/bert-base-japanese-whole-word-masking were not used when initializing BertForQuestionAnswering: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at cl-tohoku/bert-base-japanese-whole-word-masking and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "質問: 重傷者は何名？\n",
            "応答: \n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}